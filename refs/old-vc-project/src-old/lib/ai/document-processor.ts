import { multiModalDocumentAI } from "./multimodal-document-ai";
import { adkAgentServer } from "../agents/adk-agent-server";
import { llamaParseService } from "./llama-parse";
import { simplePDFExtractor } from "./simple-pdf-extractor";
import { DocumentClassifier, DocumentType } from "./document-classifier";
import { DataValidator } from "./data-validator";
import { ExtractionConfidenceService } from "./extraction-confidence";
import { RobustExtractor } from "./robust-extractor";
import { SemanticExtractor } from "./semantic-extractor";
import { getFieldsForDocumentType, getFieldConfig } from "./vc-field-configs";
import {
  DocumentProcessingResult,
  DocumentUploadRequest,
} from "../types/documents";

export class AIDocumentProcessor {
  private useMultiModal: boolean;
  private useADKAgents: boolean;
  private useLlamaParse: boolean;
  private documentClassifier: DocumentClassifier;
  private dataValidator: DataValidator;
  private confidenceService: ExtractionConfidenceService;
  private robustExtractor: RobustExtractor;
  private semanticExtractor: SemanticExtractor;

  constructor() {
    // Use multi-modal if any specialized processors are configured
    this.useMultiModal = !!(
      process.env.DOCUMENT_AI_FORM_PROCESSOR_ID ||
      process.env.DOCUMENT_AI_TABLE_PROCESSOR_ID ||
      process.env.DOCUMENT_AI_LAYOUT_PROCESSOR_ID
    );

    // Use ADK agents if Google Cloud project is configured
    this.useADKAgents = !!(
      process.env.GOOGLE_CLOUD_PROJECT_ID &&
      process.env.GOOGLE_CLOUD_PROJECT_ID !== ""
    );

    // Use Llama Parse if API key is configured
    this.useLlamaParse = !!(
      process.env.LLAMA_CLOUD_API_KEY &&
      process.env.LLAMA_CLOUD_API_KEY !== "" &&
      process.env.LLAMA_CLOUD_API_KEY !== "your_llama_cloud_api_key_here"
    );

    // Initialize services
    this.documentClassifier = new DocumentClassifier();
    this.dataValidator = new DataValidator();
    this.confidenceService = new ExtractionConfidenceService();
    this.robustExtractor = new RobustExtractor();
    this.semanticExtractor = new SemanticExtractor();
  }

  /**
   * Process a document using the appropriate AI system
   */
  async processDocument(
    request: DocumentUploadRequest
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      // Step 1: Classify document type
      console.log("üîç Classifying document type...");
      const classification = await this.classifyDocument(request);
      console.log(
        `üìã Document classified as: ${
          classification.documentType
        } (confidence: ${(classification.confidence * 100).toFixed(0)}%)`
      );

      // Step 2: Process with appropriate strategy
      let result: DocumentProcessingResult;

      if (this.useLlamaParse) {
        // Use semantic extraction for better reliability
        result = await this.processWithSemanticExtraction(
          request,
          classification
        );
      } else if (this.useADKAgents) {
        result = await this.processWithADKAgents(request);
      } else if (this.useMultiModal) {
        result = await this.processWithLlamaParse(request);
      } else {
        return {
          success: false,
          error:
            "No AI processing service configured. Please configure Llama Parse, ADK Agents, or Multi-modal Document AI.",
          processingTime: Date.now() - startTime,
        };
      }

      // Step 3: Validate and score results
      if (result.success && result.extractedData) {
        console.log("üîç Validating extracted data...");
        const validation = await this.validateExtractedData(
          result.extractedData,
          classification.documentType
        );
        const confidence = this.confidenceService.calculateConfidence(
          result.extractedData as Record<string, unknown>,
          classification.documentType
        );

        // Add validation and confidence data
        result.extractedData = {
          ...result.extractedData,
          documentClassification: classification,
          validationResults: validation,
          extractionConfidence: confidence,
          processingStrategy: classification.extractionStrategy,
        } as any;

        console.log(
          `üìä Extraction confidence: ${(
            confidence.overallConfidence * 100
          ).toFixed(0)}% (${confidence.extractionQuality})`
        );
      }

      return result;
    } catch (error) {
      console.error("‚ùå Document processing failed:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Classify document type
   */
  private async classifyDocument(request: DocumentUploadRequest) {
    // For now, use a simple classification based on file name and content
    // In production, this would analyze the actual document content
    const fileName = request.file.name.toLowerCase();

    if (fileName.includes("lp") || fileName.includes("capital account")) {
      return {
        documentType: DocumentType.LP_CAPITAL_ACCOUNT,
        confidence: 0.8,
        reasoning: "File name suggests LP capital account statement",
        extractionStrategy: ["extract_personal_lp_data", "skip_fund_metrics"],
      };
    }

    if (fileName.includes("fund") || fileName.includes("quarterly")) {
      return {
        documentType: DocumentType.FUND_QUARTERLY_REPORT,
        confidence: 0.9,
        reasoning: "File name suggests fund quarterly report",
        extractionStrategy: [
          "extract_fund_metrics",
          "extract_portfolio_companies",
          "skip_personal_lp_data",
        ],
      };
    }

    return {
      documentType: DocumentType.UNKNOWN,
      confidence: 0.3,
      reasoning: "Could not determine document type from file name",
      extractionStrategy: ["extract_all_with_low_confidence"],
    };
  }

  /**
   * Validate extracted data
   */
  private async validateExtractedData(data: any, documentType: string) {
    const fundValidation = this.dataValidator.validateFundMetrics(data);
    const portfolioValidation = data.portfolioCompanies
      ? this.dataValidator.validatePortfolioCompanies(data.portfolioCompanies)
      : null;

    return {
      fundValidation,
      portfolioValidation,
      crossValidation: portfolioValidation
        ? this.dataValidator.crossValidateFundAndPortfolio(data, {
            companies: data.portfolioCompanies,
          })
        : null,
    };
  }

  /**
   * Process document using semantic extraction (LLM-first approach)
   */
  private async processWithSemanticExtraction(
    request: DocumentUploadRequest,
    classification: any
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      console.log("üß† Starting semantic extraction with LLM-first approach...");

      // Get Llama Parse result for text content
      const llamaResult = await llamaParseService.parseDocument(request.file);
      if (!llamaResult.success || !llamaResult.markdownText) {
        throw new Error("Llama Parse failed to extract text content");
      }

      const markdownText = llamaResult.markdownText;

      // Use semantic extraction for understanding document structure and extracting data
      const extractionResult = await this.semanticExtractor.extractFromDocument(
        markdownText
      );

      if (!extractionResult.success) {
        throw new Error(
          `Semantic extraction failed: ${extractionResult.reasoning}`
        );
      }

      console.log(
        `üìä Semantic extraction completed: ${
          extractionResult.confidence * 100
        }% confidence`
      );
      console.log(`üîç Method: ${extractionResult.method}`);
      console.log(`üí≠ Reasoning: ${extractionResult.reasoning}`);

      // Add metadata
      const extractedData = {
        ...extractionResult.data,
        rawText: markdownText,
        confidence: extractionResult.confidence,
        pageCount: 1, // Llama Parse doesn't provide page count
        documentType: request.documentType,
        processingMethod: "semantic_extraction",
        extractionReasoning: extractionResult.reasoning,
        validationResults: extractionResult.validation,
      };

      return {
        success: true,
        extractedData,
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error("‚ùå Semantic extraction failed:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Process document using robust extraction (regex + LLM fallback)
   */
  private async processWithRobustExtraction(
    request: DocumentUploadRequest,
    classification: any
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      console.log("üîç Starting robust extraction with regex + LLM fallback...");

      // Get Llama Parse result for text content
      const llamaResult = await llamaParseService.parseDocument(request.file);
      if (!llamaResult.success || !llamaResult.markdownText) {
        throw new Error("Llama Parse failed to extract text content");
      }

      const markdownText = llamaResult.markdownText;
      const extractedData: Record<string, unknown> = {};

      // Get fields to extract based on document type
      const fieldsToExtract = getFieldsForDocumentType(
        classification.documentType
      );
      console.log(
        `üìã Extracting ${fieldsToExtract.length} fields for ${classification.documentType}`
      );

      // Extract each field using robust extraction
      for (const fieldName of fieldsToExtract) {
        const fieldConfig = getFieldConfig(fieldName);
        if (!fieldConfig) {
          console.warn(`‚ö†Ô∏è No configuration found for field: ${fieldName}`);
          continue;
        }

        console.log(`üîç Extracting ${fieldName}...`);
        const result = await this.robustExtractor.extractField(
          markdownText,
          fieldConfig
        );

        extractedData[fieldName] = result.value;

        console.log(
          `üìä ${fieldName}: ${result.value} (${result.method}, confidence: ${(
            result.confidence * 100
          ).toFixed(0)}%)`
        );

        // Log validation results
        if (result.validation.isMissing) {
          console.warn(`‚ö†Ô∏è ${fieldName} appears to be missing from document`);
        } else if (result.method === "llm") {
          console.log(`ü§ñ ${fieldName} extracted using LLM fallback`);
        }
      }

      // Add metadata
      extractedData.rawText = markdownText;
      extractedData.confidence = this.calculateOverallConfidence(extractedData);
      extractedData.pageCount = 1; // Llama Parse doesn't provide page count
      extractedData.documentType = request.documentType;
      extractedData.processingMethod = "robust_extraction";

      return {
        success: true,
        extractedData,
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error("‚ùå Robust extraction failed:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Calculate overall confidence from extracted data
   */
  private calculateOverallConfidence(data: Record<string, unknown>): number {
    const values = Object.values(data).filter(
      (value) => value !== null && value !== undefined && value !== ""
    );
    return values.length / Object.keys(data).length;
  }

  /**
   * Process document using Llama Parse for parsing + RAG for extraction
   */
  private async processWithLlamaParseAndRAG(
    request: DocumentUploadRequest
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      console.log(
        `Processing document with Llama Parse + RAG: ${request.file.name}`
      );

      // Step 1: Parse document with Llama Parse (multimodal parsing only)
      const llamaResult = await llamaParseService.parseDocument(request.file);

      if (!llamaResult.success || !llamaResult.markdownText) {
        return {
          success: false,
          error: llamaResult.error || "Llama Parse failed to process document",
          processingTime: Date.now() - startTime,
        };
      }

      // Step 2: Use RAG-based extraction with Supabase chunking/embedding + Google ADK
      console.log("üîÑ Using RAG-based extraction with Supabase + Google ADK");
      const extractedData = await this.extractWithRAG(
        llamaResult.markdownText,
        request.documentType,
        request.file.name
      );

      // Convert to our expected format
      return {
        success: true,
        extractedData: {
          // Basic document info
          rawText: llamaResult.markdownText,
          confidence: 0.95,
          pageCount: llamaResult.metadata?.pageCount || 0,
          documentType: request.documentType,

          // RAG-extracted data
          ...extractedData,

          // Metadata
          processingMethod: "llama_parse_rag",
          markdownText: llamaResult.markdownText,
        },
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error("Llama Parse + RAG processing error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Process document using Llama Parse
   */
  private async processWithLlamaParse(
    request: DocumentUploadRequest
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      console.log(`Processing document with Llama Parse: ${request.file.name}`);

      // Parse document with Llama Parse
      const llamaResult = await llamaParseService.parseDocument(request.file);

      if (!llamaResult.success || !llamaResult.markdownText) {
        return {
          success: false,
          error: llamaResult.error || "Llama Parse failed to process document",
          processingTime: Date.now() - startTime,
        };
      }

      // Use Llama Parse comprehensive extraction with personal LP data
      console.log(
        "üîÑ Using Llama Parse comprehensive extraction with personal LP data"
      );
      const structuredData = await llamaParseService.extractStructuredData(
        llamaResult.markdownText,
        request.documentType
      );

      // Convert to our expected format
      return {
        success: true,
        extractedData: {
          // Basic document info
          rawText: llamaResult.markdownText,
          confidence:
            structuredData.confidence ||
            llamaResult.metadata?.confidence ||
            0.95,
          pageCount: llamaResult.metadata?.pageCount || 0,
          documentType: request.documentType,

          // Enhanced structured data extracted from markdown
          companyName: structuredData.companyName,
          totalValue: structuredData.totalValue,
          irr: structuredData.irr,
          moic: structuredData.moic,
          tvpi: structuredData.tvpi,
          dpi: structuredData.dpi,
          rvpi: structuredData.rvpi,
          pic: structuredData.pic,
          investmentDate: structuredData.investmentDate,
          revenue: structuredData.revenue,
          expenses: structuredData.expenses,
          netIncome: structuredData.netIncome,
          unrealizedGains: structuredData.unrealizedGains,
          realizedGains: structuredData.realizedGains,
          activeInvestments: structuredData.activeInvestments,
          exitedInvestments: structuredData.exitedInvestments,
          portfolioCompanies: structuredData.portfolioCompanies || [],
          capitalCalls: structuredData.capitalCalls || [],
          distributions: structuredData.distributions || [],
          riskMetrics: structuredData.riskMetrics || {},
          coInvestors: structuredData.coInvestors || [],

          // Personal LP data (CRITICAL FIX)
          personalCommitment: structuredData.personalCommitment,
          personalCalledCapital: structuredData.personalCalledCapital,
          personalNAV: structuredData.personalNAV,
          personalDistributions: structuredData.personalDistributions,
          personalOwnership: structuredData.personalOwnership,
          personalMOIC: structuredData.personalMOIC,
          personalIRR: structuredData.personalIRR,
          personalUnfunded: structuredData.personalUnfunded,

          // Fund-level data
          fundSize: structuredData.fundSize,
          fundNAV: structuredData.fundNAV,
          cumulativeCalledCapital: structuredData.cumulativeCalledCapital,
          cumulativeDistributions: structuredData.cumulativeDistributions,
          unfundedCommitment: structuredData.unfundedCommitment,
          managementFeeRate: structuredData.managementFeeRate,
          carriedInterestRate: structuredData.carriedInterestRate,
          hurdleRate: structuredData.hurdleRate,

          // Tables and form fields
          tables: structuredData.tables || [],
          formFields: structuredData.formFields || [],

          // Additional metadata
          processingMethod: structuredData.processingMethod || "llama_parse",
          markdownText: llamaResult.markdownText,
        },
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error("Llama Parse processing error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Process document using ADK Agent Server
   */
  private async processWithADKAgents(
    request: DocumentUploadRequest
  ): Promise<DocumentProcessingResult> {
    const startTime = Date.now();

    try {
      // Extract text from the document first
      const extractedText = await simplePDFExtractor.extractTextFromFile(
        request.file
      );

      // Process through ADK Agent Server
      const adkResult = await adkAgentServer.processDocument({
        fileName: request.file.name,
        fileType: request.file.type,
        fileSize: request.file.size,
        rawText: extractedText.rawText,
        userId: request.userId,
        portfolioId: request.portfolioId,
      });

      // Convert ADK result to our format
      return {
        success: adkResult.success,
        extractedData: adkResult.extractedData
          ? ({
              // Map ADK result to our interface
              documentType: adkResult.extractedData.documentType,
              confidence: adkResult.extractedData.confidence,
              rawText: adkResult.extractedData.rawText,
              pageCount: adkResult.extractedData.pageCount,

              // Fund Report fields - Basic metrics
              totalValue: adkResult.extractedData.totalValue,
              irr: adkResult.extractedData.irr,
              moic: adkResult.extractedData.moic,
              tvpi: (adkResult.extractedData as any).tvpi,
              dpi: (adkResult.extractedData as any).dpi,
              rvpi: (adkResult.extractedData as any).rvpi,
              investmentDate: adkResult.extractedData.investmentDate,
              companyName: adkResult.extractedData.companyName,

              // Fund-level metrics (CRITICAL FIX)
              fundSize: (adkResult.extractedData as any).fundSize,
              fundNAV: (adkResult.extractedData as any).fundNAV,
              cumulativeCalledCapital: (adkResult.extractedData as any)
                .cumulativeCalledCapital,
              cumulativeDistributions: (adkResult.extractedData as any)
                .cumulativeDistributions,
              unfundedCommitment: (adkResult.extractedData as any)
                .unfundedCommitment,
              managementFeeRate: (adkResult.extractedData as any)
                .managementFeeRate,
              carriedInterestRate: (adkResult.extractedData as any)
                .carriedInterestRate,
              hurdleRate: (adkResult.extractedData as any).hurdleRate,

              // Personal investor metrics (CRITICAL FIX)
              personalCommitment: (adkResult.extractedData as any)
                .personalCommitment,
              personalCalledCapital: (adkResult.extractedData as any)
                .personalCalledCapital,
              personalNAV: (adkResult.extractedData as any).personalNAV,
              personalDistributions: (adkResult.extractedData as any)
                .personalDistributions,
              personalOwnership: (adkResult.extractedData as any)
                .personalOwnership,
              personalUnfunded: (adkResult.extractedData as any)
                .personalUnfunded,
              personalMOIC: (adkResult.extractedData as any).personalMOIC,
              personalIRR: (adkResult.extractedData as any).personalIRR,

              // Portfolio companies and other data
              portfolioCompanies:
                (adkResult.extractedData as any).portfolioCompanies || [],
              capitalCalls: (adkResult.extractedData as any).capitalCalls || [],
              distributions:
                (adkResult.extractedData as any).distributions || [],
              riskMetrics: (adkResult.extractedData as any).riskMetrics || {},
              coInvestors: (adkResult.extractedData as any).coInvestors || [],

              // Cap Table fields
              ownershipPercentage: (adkResult.extractedData as any)
                .ownershipPercentage,
              totalShares: (adkResult.extractedData as any).totalShares,

              // Financial Statement fields
              revenue: (adkResult.extractedData as any).revenue,
              expenses: (adkResult.extractedData as any).expenses,
              netIncome: (adkResult.extractedData as any).netIncome,
              assets: (adkResult.extractedData as any).assets,
              liabilities: (adkResult.extractedData as any).liabilities,

              // Multi-modal data (for frontend compatibility)
              tables: (adkResult.extractedData as any).tables || [],
              formFields: (adkResult.extractedData as any).formFields || [],
              entities: (adkResult.extractedData as any).entities || [],
              layout: (adkResult.extractedData as any).layout || {
                pages: [{ pageNumber: 1, blocks: [] }],
              },
            } as any)
          : undefined,
        error: adkResult.success ? undefined : "ADK Agent processing failed",
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error("ADK Agent processing error:", error);
      return {
        success: false,
        error: `ADK Agent processing failed: ${
          error instanceof Error ? error.message : "Unknown error"
        }`,
        processingTime: Date.now() - startTime,
      };
    }
  }

  /**
   * Extract structured data using RAG approach with vector search and targeted queries
   */
  private async extractWithRAG(
    markdownText: string,
    documentType: string,
    fileName: string
  ): Promise<Record<string, unknown>> {
    console.log(`üîç Starting RAG extraction for ${fileName}`);

    try {
      // Step 1: Chunk and embed document in Supabase
      console.log("üìö Chunking and embedding document in Supabase...");
      const chunks = await this.chunkDocument(markdownText);
      const embeddings = await this.createEmbeddings(chunks);

      // Step 2: Use Google ADK for vector search and RAG
      console.log("üîç Using Google ADK for vector search and RAG...");
      const personalLPData = await this.queryPersonalLPDataWithRAG(
        embeddings,
        markdownText
      );
      const fundData = await this.queryFundDataWithRAG(
        embeddings,
        markdownText
      );
      const portfolioData = await this.queryPortfolioDataWithRAG(
        embeddings,
        markdownText
      );

      const result: Record<string, unknown> = {
        // Basic document info
        rawText: markdownText,
        confidence: 0.95,
        pageCount: 1,
        documentType,

        // Personal LP data (RAG-extracted)
        personalCommitment: personalLPData.commitment || null,
        personalCalledCapital: personalLPData.calledCapital || null,
        personalNAV: personalLPData.nav || null,
        personalDistributions: personalLPData.distributions || null,
        personalOwnership: personalLPData.ownership || null,
        personalUnfunded: personalLPData.unfunded || null,
        personalMOIC: personalLPData.moic || null,
        personalIRR: personalLPData.irr || null,

        // Fund-level data (RAG-extracted)
        companyName: fundData.companyName || null,
        totalValue: fundData.fundSize || null,
        fundSize: fundData.fundSize || null,
        fundNAV: fundData.fundNAV || null,
        cumulativeCalledCapital: fundData.cumulativeCalledCapital || null,
        totalCommitments: fundData.totalCommitments || null,
        uncalledCapital: fundData.uncalledCapital || null,
        irr: fundData.irr || null,
        moic: fundData.moic || null,
        tvpi: fundData.tvpi || null,
        dpi: fundData.dpi || null,
        rvpi: fundData.rvpi || null,

        // Portfolio data (RAG-extracted)
        portfolioCompanies: portfolioData.companies || [],
        capitalCalls: portfolioData.capitalCalls || [],
        distributions: portfolioData.distributions || [],

        // Empty arrays for multimodal fields
        tables: [],
        formFields: [],
        entities: [],

        // Metadata
        processingMethod: "RAG",
      };

      return result;
    } catch (error) {
      console.error("RAG extraction failed:", error);
      // Fallback to semantic analysis
      return await this.extractWithSemanticAnalysis(markdownText, documentType);
    }
  }

  /**
   * Chunk document for embedding
   */
  private async chunkDocument(markdownText: string): Promise<string[]> {
    // Split document into semantic chunks
    const chunks: string[] = [];
    const sections = markdownText.split(/\n#{1,6}\s+/);

    for (const section of sections) {
      if (section.trim().length > 50) {
        // Only include substantial sections
        chunks.push(section.trim());
      }
    }

    return chunks;
  }

  /**
   * Create embeddings using Supabase
   */
  private async createEmbeddings(chunks: string[]): Promise<any[]> {
    // This would integrate with Supabase's embedding service
    // For now, return mock embeddings
    return chunks.map((chunk, index) => ({
      id: `chunk_${index}`,
      content: chunk,
      embedding: new Array(1536).fill(0).map(() => Math.random()), // Mock embedding
    }));
  }

  /**
   * Query personal LP data using RAG with Google ADK
   */
  private async queryPersonalLPDataWithRAG(
    embeddings: unknown[],
    markdownText: string
  ): Promise<Record<string, unknown>> {
    console.log("üîç Querying personal LP data using RAG approach");

    try {
      // Use Google ADK for vector search and RAG
      const relevantChunks = await this.performVectorSearch(
        embeddings,
        "personal LP data commitment called capital NAV distributions"
      );

      // Extract data from relevant chunks using semantic analysis
      const result = await this.extractPersonalLPFromChunks(relevantChunks);
      console.log("‚úÖ RAG personal LP extraction completed:", result);
      return result;
    } catch (error) {
      console.error("‚ùå RAG personal LP extraction failed:", error);
      // Fallback to direct semantic extraction
      return await this.extractPersonalLPFromMarkdown(markdownText);
    }
  }

  /**
   * Query fund-level data using RAG
   */
  private async queryFundDataWithRAG(
    embeddings: unknown[],
    _markdownText: string
  ): Promise<Record<string, unknown>> {
    try {
      const relevantChunks = await this.performVectorSearch(
        embeddings,
        "fund performance IRR MOIC TVPI DPI RVPI fund size NAV"
      );

      return await this.extractFundDataFromChunks(relevantChunks);
    } catch (error) {
      console.error("Fund data RAG extraction failed:", error);
      return {
        companyName: null,
        fundSize: null,
        fundNAV: null,
        irr: null,
        moic: null,
        tvpi: null,
        dpi: null,
        rvpi: null,
      };
    }
  }

  /**
   * Query portfolio data using RAG
   */
  private async queryPortfolioDataWithRAG(
    embeddings: unknown[],
    _markdownText: string
  ): Promise<Record<string, unknown>> {
    try {
      const relevantChunks = await this.performVectorSearch(
        embeddings,
        "portfolio companies capital calls distributions investments"
      );

      return await this.extractPortfolioDataFromChunks(relevantChunks);
    } catch (error) {
      console.error("Portfolio data RAG extraction failed:", error);
      return {
        companies: [],
        capitalCalls: [],
        distributions: [],
      };
    }
  }

  /**
   * Perform vector search using Google ADK
   */
  private async performVectorSearch(
    embeddings: unknown[],
    _query: string
  ): Promise<unknown[]> {
    // This would use Google ADK for vector search
    // For now, return mock relevant chunks
    return embeddings.slice(0, 3); // Return first 3 chunks as mock
  }

  /**
   * Extract personal LP data from relevant chunks
   */
  private async extractPersonalLPFromChunks(chunks: any[]): Promise<any> {
    const result: any = {};

    for (const chunk of chunks) {
      const chunkData = await this.extractPersonalLPFromMarkdown(chunk.content);
      Object.assign(result, chunkData);
    }

    return result;
  }

  /**
   * Extract fund data from relevant chunks
   */
  private async extractFundDataFromChunks(
    chunks: unknown[]
  ): Promise<Record<string, unknown>> {
    try {
      // Combine all chunk content for analysis
      const combinedText = chunks
        .map(
          (chunk) =>
            (chunk as Record<string, unknown>).content ||
            (chunk as Record<string, unknown>).text ||
            ""
        )
        .join("\n");

      // Extract fund-level metrics using pattern matching and LLM analysis
      const fundData = await this.parseFundMetrics(combinedText);

      return {
        companyName: fundData.companyName || null,
        fundSize: fundData.fundSize || null,
        fundNAV: fundData.fundNAV || null,
        irr: fundData.irr || null,
        moic: fundData.moic || null,
        tvpi: fundData.tvpi || null,
        dpi: fundData.dpi || null,
        rvpi: fundData.rvpi || null,
        cumulativeCalledCapital: fundData.cumulativeCalledCapital || null,
        totalCommitments: fundData.totalCommitments || null,
        uncalledCapital: fundData.uncalledCapital || null,
      };
    } catch (error) {
      console.error("Error extracting fund data from chunks:", error);
      return {
        companyName: null,
        fundSize: null,
        fundNAV: null,
        irr: null,
        moic: null,
        tvpi: null,
        dpi: null,
        rvpi: null,
        cumulativeCalledCapital: null,
        totalCommitments: null,
        uncalledCapital: null,
      };
    }
  }

  /**
   * Parse fund metrics from text using language-agnostic methods
   */
  private async parseFundMetrics(
    text: string
  ): Promise<Record<string, unknown>> {
    try {
      const fundData: Record<string, unknown> = {};

      // Extract fund name (company name)
      const fundNameMatch = text.match(
        /(?:Fund|Fund II|Fund III|Fund IV|Fund V|Fund VI|Fund VII|Fund VIII|Fund IX|Fund X)\s+([A-Za-z\s]+)/i
      );
      if (fundNameMatch) {
        fundData.companyName = fundNameMatch[1].trim();
      }

      // Extract total commitments
      const totalCommitmentsMatch = text.match(
        /Total committed[:\s]*([0-9,]+\.?[0-9]*)\s*(?:kEUR|EUR|‚Ç¨|k‚Ç¨)/i
      );
      if (totalCommitmentsMatch) {
        fundData.totalCommitments = parseFloat(
          totalCommitmentsMatch[1].replace(/,/g, "")
        );
        fundData.fundSize = fundData.totalCommitments; // Alias for compatibility
      }

      // Extract NAV
      const navMatch = text.match(
        /Net asset value[:\s]*\(NAV\)[:\s]*([0-9,]+\.?[0-9]*)\s*(?:kEUR|EUR|‚Ç¨|k‚Ç¨)/i
      );
      if (navMatch) {
        fundData.fundNAV = parseFloat(navMatch[1].replace(/,/g, ""));
      }

      // Extract IRR
      const irrMatch = text.match(
        /IRR[,\s]*(?:gross|net)[:\s]*([0-9]+\.?[0-9]*)\s*%/i
      );
      if (irrMatch) {
        fundData.irr = parseFloat(irrMatch[1]);
      }

      // Extract MOIC
      const moicMatch = text.match(
        /Multiple to investors[:\s]*([0-9]+\.?[0-9]*)\s*x/i
      );
      if (moicMatch) {
        fundData.moic = parseFloat(moicMatch[1]);
      }

      // Extract TVPI
      const tvpiMatch = text.match(
        /Total value to paid-in capital[:\s]*\(TVPI\)[:\s]*([0-9]+\.?[0-9]*)\s*x/i
      );
      if (tvpiMatch) {
        fundData.tvpi = parseFloat(tvpiMatch[1]);
      }

      // Extract DPI
      const dpiMatch = text.match(
        /Distributions to paid-in capital[:\s]*\(DPI\)[:\s]*([0-9]+\.?[0-9]*)\s*x/i
      );
      if (dpiMatch) {
        fundData.dpi = parseFloat(dpiMatch[1]);
      }

      // Extract RVPI
      const rvpiMatch = text.match(
        /Residual value to paid-in capital[:\s]*\(RVPI\)[:\s]*([0-9]+\.?[0-9]*)\s*x/i
      );
      if (rvpiMatch) {
        fundData.rvpi = parseFloat(rvpiMatch[1]);
      }

      // Extract cumulative called capital
      const calledCapitalMatch = text.match(
        /Total capital calls[:\s]*\/\s*in\s*%\s*of\s*committed\s*capital[:\s]*([0-9,]+\.?[0-9]*)\s*(?:kEUR|EUR|‚Ç¨|k‚Ç¨)/i
      );
      if (calledCapitalMatch) {
        fundData.cumulativeCalledCapital = parseFloat(
          calledCapitalMatch[1].replace(/,/g, "")
        );
      }

      // Extract uncalled capital
      const uncalledMatch = text.match(
        /Uncalled capital[:\s]*\/\s*in\s*%\s*of\s*committed\s*capital[:\s]*([0-9,]+\.?[0-9]*)\s*(?:kEUR|EUR|‚Ç¨|k‚Ç¨)/i
      );
      if (uncalledMatch) {
        fundData.uncalledCapital = parseFloat(
          uncalledMatch[1].replace(/,/g, "")
        );
      }

      return fundData;
    } catch (error) {
      console.error("Error parsing fund metrics:", error);
      return {};
    }
  }

  /**
   * Extract portfolio data from relevant chunks
   */
  private async extractPortfolioDataFromChunks(
    chunks: unknown[]
  ): Promise<Record<string, unknown>> {
    try {
      // Combine all chunk content for analysis
      const combinedText = chunks
        .map(
          (chunk) =>
            (chunk as Record<string, unknown>).content ||
            (chunk as Record<string, unknown>).text ||
            ""
        )
        .join("\n");

      // Extract portfolio companies using pattern matching
      const portfolioData = await this.parsePortfolioCompanies(combinedText);

      return {
        companies: portfolioData.companies || [],
        capitalCalls: portfolioData.capitalCalls || [],
        distributions: portfolioData.distributions || [],
      };
    } catch (error) {
      console.error("Error extracting portfolio data from chunks:", error);
      return {
        companies: [],
        capitalCalls: [],
        distributions: [],
      };
    }
  }

  /**
   * Parse portfolio companies from text
   */
  private async parsePortfolioCompanies(
    text: string
  ): Promise<Record<string, unknown>> {
    try {
      const companies: Record<string, unknown>[] = [];

      // Extract company names and basic info from portfolio tables
      const companyMatches = text.match(
        /\| ([A-Za-z\s]+(?:GmbH|Ltd|Inc|Corp|LLC|AG|KG|GmbH & Co\. KG))\s+\|/g
      );
      if (companyMatches) {
        companyMatches.forEach((match) => {
          const companyName = match.replace(/\|/g, "").trim();
          if (companyName && companyName.length > 2) {
            companies.push({
              name: companyName,
              stage: "Unknown",
              sector: "Unknown",
              valuation: null,
              investment: null,
              moic: null,
              irr: null,
            });
          }
        });
      }

      return {
        companies,
        capitalCalls: [],
        distributions: [],
      };
    } catch (error) {
      console.error("Error parsing portfolio companies:", error);
      return {
        companies: [],
        capitalCalls: [],
        distributions: [],
      };
    }
  }

  /**
   * Extract personal LP data using LLM-based approach (no regex patterns)
   */
  private async extractPersonalLPFromMarkdown(
    _markdownText: string
  ): Promise<Record<string, unknown>> {
    console.log("üîç Extracting personal LP data using LLM-based approach");

    try {
      // Use LLM to extract personal LP data from the markdown content
      // Note: This would use an LLM service to extract personal LP data
      // For now, return empty data as this document doesn't contain personal LP data

      // For now, return empty data as this is a fund-level report
      return {
        commitment: null,
        calledCapital: null,
        nav: null,
        distributions: null,
        unfunded: null,
        ownership: null,
        moic: null,
        irr: null,
      };
    } catch (error) {
      console.error("‚ùå LLM extraction failed:", error);
      return {};
    }
  }

  /**
   * Semantic extraction using document structure analysis (no regex)
   */
  private async semanticExtractPersonalLPData(
    markdownText: string
  ): Promise<any> {
    console.log("üß† Using semantic analysis for personal LP data extraction");

    const result: any = {};

    // Find the Individual capital account section using semantic analysis
    const sections = this.identifyDocumentSections(markdownText);
    const individualCapitalSection = sections.find(
      (section) =>
        section.title.toLowerCase().includes("individual capital account") ||
        section.title.toLowerCase().includes("investor statement") ||
        section.title.toLowerCase().includes("inception to") ||
        section.title.toLowerCase().includes("capital account")
    );

    if (individualCapitalSection) {
      console.log("üìã Found Individual capital account section");

      // Extract data using table structure analysis
      const tableData = this.analyzeTableStructure(
        individualCapitalSection.content
      );

      // Map table data to personal LP metrics using semantic understanding
      for (const row of tableData) {
        const rowText = row.join(" ").toLowerCase();

        if (rowText.includes("commitment") && row.length > 4) {
          const value = this.extractNumericValue(row[row.length - 2]);
          if (value !== null) {
            result.commitment = value;
            console.log(`‚úÖ Extracted commitment: ${result.commitment}`);
          }
        } else if (rowText.includes("paid in capital") && row.length > 4) {
          const value = this.extractNumericValue(row[row.length - 2]);
          if (value !== null) {
            result.calledCapital = value;
            console.log(`‚úÖ Extracted called capital: ${result.calledCapital}`);
          }
        } else if (rowText.includes("capital account") && row.length > 4) {
          const value = this.extractNumericValue(row[row.length - 2]);
          if (value !== null) {
            result.nav = value;
            console.log(`‚úÖ Extracted NAV: ${result.nav}`);
          }
        } else if (rowText.includes("unfunded") && row.length > 4) {
          const value = this.extractNumericValue(row[row.length - 2]);
          if (value !== null) {
            result.unfunded = value;
            console.log(`‚úÖ Extracted unfunded: ${result.unfunded}`);
          }
        } else if (rowText.includes("ownership") && row.length > 4) {
          const value = this.extractNumericValue(row[row.length - 2]);
          if (value !== null) {
            result.ownership = value / 100; // Convert percentage to decimal
            console.log(`‚úÖ Extracted ownership: ${result.ownership}`);
          }
        }
      }
    }

    return result;
  }

  /**
   * Identify document sections using semantic analysis
   */
  private identifyDocumentSections(
    markdownText: string
  ): Array<{ title: string; content: string }> {
    const sections: Array<{ title: string; content: string }> = [];
    const lines = markdownText.split("\n");

    let currentSection = { title: "", content: "" };

    for (const line of lines) {
      if (line.startsWith("# ")) {
        if (currentSection.title) {
          sections.push(currentSection);
        }
        currentSection = { title: line.substring(2), content: "" };
      } else {
        currentSection.content += line + "\n";
      }
    }

    if (currentSection.title) {
      sections.push(currentSection);
    }

    // Also look for sections that might contain the table data
    // Find the section that contains the table data
    const tableSection = sections.find(
      (section) =>
        section.content.includes("|") &&
        section.content.includes("INVESTOR") &&
        section.content.includes("Commitment")
    );

    if (tableSection) {
      console.log(`üìã Found table section: ${tableSection.title}`);
      return [tableSection];
    }

    return sections;
  }

  /**
   * Analyze table structure using semantic understanding
   */
  private analyzeTableStructure(content: string): string[][] {
    const rows: string[][] = [];
    const lines = content.split("\n");

    for (const line of lines) {
      if (line.includes("|") && line.trim().length > 0) {
        const columns = line.split("|").map((col) => col.trim());
        // Filter out empty columns but keep the structure
        const filteredColumns = columns.filter((col) => col !== "");
        if (filteredColumns.length > 1) {
          rows.push(filteredColumns);
        }
      }
    }

    console.log(`üìä Analyzed table structure: ${rows.length} rows found`);
    return rows;
  }

  /**
   * Extract numeric value from text using semantic analysis
   */
  private extractNumericValue(text: string): number | null {
    if (!text || text === "-") return null;

    // Remove common formatting and extract number
    const cleanText = text.replace(/[‚Ç¨$,\s]/g, "");
    const number = parseFloat(cleanText);

    return isNaN(number) ? null : number;
  }

  /**
   * Fallback semantic-based extraction (no regex patterns)
   */
  private async extractWithSemanticAnalysis(
    markdownText: string,
    documentType: string
  ): Promise<any> {
    console.log("üîÑ Using fallback semantic analysis extraction");

    try {
      // Use the same semantic extraction approach as the main method
      const result = await this.semanticExtractPersonalLPData(markdownText);
      console.log("‚úÖ Semantic extraction completed:", result);
      return result;
    } catch (error) {
      console.error("‚ùå Semantic extraction failed:", error);
      // Return basic structure to prevent processing failure
      return {
        commitment: null,
        calledCapital: null,
        nav: null,
        ownership: null,
        unfunded: null,
        moic: null,
        irr: null,
      };
    }
  }
}

// Export singleton instance
export const documentProcessor = new AIDocumentProcessor();
